{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3f609f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpandas\u001b[49m\n\u001b[0;32m      2\u001b[0m requests\n\u001b[0;32m      3\u001b[0m beautifulsoup4\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pandas' is not defined"
     ]
    }
   ],
   "source": [
    "pandas\n",
    "requests\n",
    "beautifulsoup4\n",
    "openpyxl\n",
    "textblob\n",
    "nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fab873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "Collecting nltk>=3.8\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\anaconda\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from nltk>=3.8->textblob) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda\\lib\\site-packages (from nltk>=3.8->textblob) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.4)\n",
      "Installing collected packages: nltk, textblob\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.8.1 textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db672458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "183ffa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application\n",
      "Healthcare AI ChatBot using LLAMA, LLM, Langchain Efficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application Development Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment AI Chatbot using LLM, Langchain, LLama AI Bot Audio to audio Methodology for ETL Discovery Tool using LLMA, OpenAI, Langchain Methodology for database discovery tool using openai, LLMA, Langchain Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040. Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in Future Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways Rise of Cybercrime and its Effect in upcoming Future AI/ML and Predictive Modeling Solution for Contact Centre Problems How to Setup Custom Domain for Google App Engine Application? Code Review Checklist Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Banking, Financing Organization Size: 100+ Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. This solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy We provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth global insights into the big data,data-driven dashboards, applications development, and information management for organizations through combining unique, specialist services and high-lvel human expertise. Contact us: hello@blackcoffer.com © All Right Reserved, Blackcoffer(OPC) Pvt. Ltd\n"
     ]
    }
   ],
   "source": [
    "def extract_article_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1').get_text() if soup.find('h1') else ''\n",
    "    paragraphs = soup.find_all('p')\n",
    "    article_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "    return title + '\\n' + article_text\n",
    "\n",
    "# Example usage\n",
    "url ='https://insights.blackcoffer.com/streamlined-integration-interactive-brokers-api-with-python-for-desktop-trading-application/'\n",
    "article_text = extract_article_text(url)\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "883a3f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_count': 1, 'positive_score': 0, 'negative_score': 0, 'polarity_score': 0.0, 'subjectivity_score': 0.0, 'avg_sentence_length': 1.0, 'percentage_of_complex_words': 1.0, 'fog_index': 0.8, 'complex_word_count': 1, 'syllables_per_word': 4.0, 'personal_pronouns': 0, 'avg_word_length': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Md\n",
      "[nltk_data]     Rashid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Md Rashid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "positive_words = set(open('positive-words.txt').read().split())\n",
    "negative_words = set(open('negative-words.txt').read().split())\n",
    "\n",
    "def analyze_text(text):\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    word_count = len(words)\n",
    "    \n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    avg_sentence_length = word_count / len(sentences) if sentences else 0\n",
    "    \n",
    "    def syllables_per_word(word):\n",
    "        return len([char for char in word if char in 'aeiouAEIOU'])\n",
    "    \n",
    "    syllable_counts = [syllables_per_word(word) for word in words]\n",
    "    complex_words_count = sum(1 for count in syllable_counts if count > 2)\n",
    "    percentage_of_complex_words = complex_words_count / word_count if word_count else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "    \n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count if word_count else 0\n",
    "    \n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'positive_score': positive_score,\n",
    "        'negative_score': negative_score,\n",
    "        'polarity_score': polarity_score,\n",
    "        'subjectivity_score': subjectivity_score,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'percentage_of_complex_words': percentage_of_complex_words,\n",
    "        'fog_index': fog_index,\n",
    "        'complex_word_count': complex_words_count,\n",
    "        'syllables_per_word': sum(syllable_counts) / word_count if word_count else 0,\n",
    "        'personal_pronouns': personal_pronouns,\n",
    "        'avg_word_length': avg_word_length\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "\n",
    "analysis_results = analyze_text('')\n",
    "print(analysis_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5c922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e39b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
